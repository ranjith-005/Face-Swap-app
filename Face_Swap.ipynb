{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOdJJ2ecbSCg"
   },
   "source": [
    "# Face Swap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbBqknZ_bWoo"
   },
   "source": [
    "In this project, we will be using opencv and dlib to extract faces of human-beings from a given image. We will use a pretrained model to extract landmarks from the faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFp7MeUuJRH5"
   },
   "outputs": [],
   "source": [
    "# Library imports\r\n",
    "import cv2\r\n",
    "import numpy as np\r\n",
    "import dlib\r\n",
    "import requests  \r\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXJazfYEWLv0"
   },
   "source": [
    "We will start by importing some required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ELAu6ZHP2hZ"
   },
   "outputs": [],
   "source": [
    "# Downloading shape_predictor\r\n",
    "!wget 'https://github.com/tzutalin/dlib-android/raw/master/data/shape_predictor_68_face_landmarks.dat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Picb8eFWVeK"
   },
   "source": [
    "Now we will download pretrained model shape_predictor from github into our google collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOoqrAdYKGJ2"
   },
   "outputs": [],
   "source": [
    "# Extracting index from array\r\n",
    "def extract_index_nparray(nparray):\r\n",
    "    index = None\r\n",
    "    for num in nparray[0]:\r\n",
    "        index = num\r\n",
    "        break\r\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqIAab6JXCpo"
   },
   "source": [
    "Now we will create a function to extract the index from the numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOumnR2oNTuS"
   },
   "outputs": [],
   "source": [
    "# Reading source image form url\r\n",
    "image1 = Image.open(requests.get('https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSx8Pu1tW1uCiZPfj9K1EL6uHxbg3bOKO9XkA&usqp=CAU', stream=True).raw)\r\n",
    "image1 = image1.resize((300,300))\r\n",
    "image1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNA1qmsTXNzB"
   },
   "source": [
    "Next we will load our source image from the internet using url and resize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMvalrv0O3he"
   },
   "outputs": [],
   "source": [
    "# Reading destination image form url\r\n",
    "image2 = Image.open(requests.get('https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTYX1dyl9INRo5cbvDeTILRcZVzfcMsCsE0kg&usqp=CAU', stream=True).raw)\r\n",
    "image2 = image2.resize((300,300))\r\n",
    "image2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8w4LQ6IXb0g"
   },
   "source": [
    "Here we will load our destination image from the internet using url and resize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6bMoYV_KI06"
   },
   "outputs": [],
   "source": [
    "# Converting image to array and converting them to grayscale\r\n",
    "img = np.array(image1)\r\n",
    "img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\r\n",
    "mask = np.zeros_like(img_gray)\r\n",
    "img2 = np.array(image2)\r\n",
    "img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1i-29-leXjks"
   },
   "source": [
    "Now we will convert our images into numpy array and use cv2 to convert it into grayscale. We will also create empty image or mask similar to our source image with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jjFhYnojKNvc"
   },
   "outputs": [],
   "source": [
    "# Initalizing frontal face detector and shape predictor\r\n",
    "detector = dlib.get_frontal_face_detector()\r\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\r\n",
    "height, width, channels = img2.shape\r\n",
    "img2_new_face = np.zeros((height, width, channels), np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3qJ_JOoYB7r"
   },
   "source": [
    "Here we will first load Face detector and Face landmarks predictor using dlib and then we will find the height, width, channels which are required for creating empty image with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0alfXlyNKSsR"
   },
   "outputs": [],
   "source": [
    "# Face 1\r\n",
    "faces = detector(img_gray)\r\n",
    "for face in faces:\r\n",
    "    landmarks = predictor(img_gray, face)\r\n",
    "    landmarks_points = []\r\n",
    "    for n in range(0, 68):\r\n",
    "        x = landmarks.part(n).x\r\n",
    "        y = landmarks.part(n).y\r\n",
    "        landmarks_points.append((x, y))\r\n",
    "\r\n",
    "    points = np.array(landmarks_points, np.int32)\r\n",
    "    convexhull = cv2.convexHull(points)\r\n",
    "    cv2.fillConvexPoly(mask, convexhull, 255)\r\n",
    "\r\n",
    "    face_image_1 = cv2.bitwise_and(img, img, mask=mask)\r\n",
    "\r\n",
    "    # Delaunay triangulation\r\n",
    "    rect = cv2.boundingRect(convexhull)\r\n",
    "    subdiv = cv2.Subdiv2D(rect)\r\n",
    "    subdiv.insert(landmarks_points)\r\n",
    "    triangles = subdiv.getTriangleList()\r\n",
    "    triangles = np.array(triangles, dtype=np.int32)\r\n",
    "\r\n",
    "    indexes_triangles = []\r\n",
    "    for t in triangles:\r\n",
    "        pt1 = (t[0], t[1])\r\n",
    "        pt2 = (t[2], t[3])\r\n",
    "        pt3 = (t[4], t[5])\r\n",
    "\r\n",
    "\r\n",
    "        index_pt1 = np.where((points == pt1).all(axis=1))\r\n",
    "        index_pt1 = extract_index_nparray(index_pt1)\r\n",
    "\r\n",
    "        index_pt2 = np.where((points == pt2).all(axis=1))\r\n",
    "        index_pt2 = extract_index_nparray(index_pt2)\r\n",
    "\r\n",
    "        index_pt3 = np.where((points == pt3).all(axis=1))\r\n",
    "        index_pt3 = extract_index_nparray(index_pt3)\r\n",
    "\r\n",
    "        if index_pt1 is not None and index_pt2 is not None and index_pt3 is not None:\r\n",
    "            triangle = [index_pt1, index_pt2, index_pt3]\r\n",
    "            indexes_triangles.append(triangle)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ep6npRLbYvb8"
   },
   "source": [
    "First we need to pass image to detector then that object will used to extract landmarks using predictor. Then storing extracted landmarks points x and y into landmarks points list. We're going then to segment the face into triangles. This step is the core of our face swapping, as later we will simply exchange each triangle with the correspondent triangle of the destination image.\r\n",
    "The triangulation of the destination image needs to have the same patterns of the triangulation of the source image. That means that the connection of the points has to be the same. So after we do the triangulation of the source image, from that triangulation we take the indexes of the landmark points so that we can replicate the same triangulation on the destination image.\r\n",
    "Once we have the triangles indexes we loop through them and we triangulate the destination face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSnuuS0xKVRn"
   },
   "outputs": [],
   "source": [
    "# Face 2\r\n",
    "faces2 = detector(img2_gray)\r\n",
    "for face in faces2:\r\n",
    "    landmarks = predictor(img2_gray, face)\r\n",
    "    landmarks_points2 = []\r\n",
    "    for n in range(0, 68):\r\n",
    "        x = landmarks.part(n).x\r\n",
    "        y = landmarks.part(n).y\r\n",
    "        landmarks_points2.append((x, y))\r\n",
    "\r\n",
    "\r\n",
    "    points2 = np.array(landmarks_points2, np.int32)\r\n",
    "    convexhull2 = cv2.convexHull(points2)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlttVbJ2bEmC"
   },
   "source": [
    "Now we will apply similar function as done for source images to extract landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cw5j2VVLKX9D"
   },
   "outputs": [],
   "source": [
    "# Creating empty mask\r\n",
    "lines_space_mask = np.zeros_like(img_gray)\r\n",
    "lines_space_new_face = np.zeros_like(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "saE00buZbfsT"
   },
   "source": [
    "Here we will create empty images with zeros which will be required for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGsw_KVnKY1d"
   },
   "outputs": [],
   "source": [
    "# Triangulation of both faces\r\n",
    "for triangle_index in indexes_triangles:\r\n",
    "    # Triangulation of the first face\r\n",
    "    tr1_pt1 = landmarks_points[triangle_index[0]]\r\n",
    "    tr1_pt2 = landmarks_points[triangle_index[1]]\r\n",
    "    tr1_pt3 = landmarks_points[triangle_index[2]]\r\n",
    "    triangle1 = np.array([tr1_pt1, tr1_pt2, tr1_pt3], np.int32)\r\n",
    "\r\n",
    "\r\n",
    "    rect1 = cv2.boundingRect(triangle1)\r\n",
    "    (x, y, w, h) = rect1\r\n",
    "    cropped_triangle = img[y: y + h, x: x + w]\r\n",
    "    cropped_tr1_mask = np.zeros((h, w), np.uint8)\r\n",
    "\r\n",
    "\r\n",
    "    points = np.array([[tr1_pt1[0] - x, tr1_pt1[1] - y],\r\n",
    "                       [tr1_pt2[0] - x, tr1_pt2[1] - y],\r\n",
    "                       [tr1_pt3[0] - x, tr1_pt3[1] - y]], np.int32)\r\n",
    "\r\n",
    "    cv2.fillConvexPoly(cropped_tr1_mask, points, 255)\r\n",
    "\r\n",
    "    # Lines space\r\n",
    "    cv2.line(lines_space_mask, tr1_pt1, tr1_pt2, 255)\r\n",
    "    cv2.line(lines_space_mask, tr1_pt2, tr1_pt3, 255)\r\n",
    "    cv2.line(lines_space_mask, tr1_pt1, tr1_pt3, 255)\r\n",
    "    lines_space = cv2.bitwise_and(img, img, mask=lines_space_mask)\r\n",
    "\r\n",
    "    # Triangulation of second face\r\n",
    "    tr2_pt1 = landmarks_points2[triangle_index[0]]\r\n",
    "    tr2_pt2 = landmarks_points2[triangle_index[1]]\r\n",
    "    tr2_pt3 = landmarks_points2[triangle_index[2]]\r\n",
    "    triangle2 = np.array([tr2_pt1, tr2_pt2, tr2_pt3], np.int32)\r\n",
    "\r\n",
    "\r\n",
    "    rect2 = cv2.boundingRect(triangle2)\r\n",
    "    (x, y, w, h) = rect2\r\n",
    "\r\n",
    "    cropped_tr2_mask = np.zeros((h, w), np.uint8)\r\n",
    "\r\n",
    "    points2 = np.array([[tr2_pt1[0] - x, tr2_pt1[1] - y],\r\n",
    "                        [tr2_pt2[0] - x, tr2_pt2[1] - y],\r\n",
    "                        [tr2_pt3[0] - x, tr2_pt3[1] - y]], np.int32)\r\n",
    "\r\n",
    "    cv2.fillConvexPoly(cropped_tr2_mask, points2, 255)\r\n",
    "\r\n",
    "    # Warp triangles\r\n",
    "    points = np.float32(points)\r\n",
    "    points2 = np.float32(points2)\r\n",
    "    M = cv2.getAffineTransform(points, points2)\r\n",
    "    warped_triangle = cv2.warpAffine(cropped_triangle, M, (w, h))\r\n",
    "    warped_triangle = cv2.bitwise_and(warped_triangle, warped_triangle, mask=cropped_tr2_mask)\r\n",
    "\r\n",
    "    # Reconstructing destination face\r\n",
    "    img2_new_face_rect_area = img2_new_face[y: y + h, x: x + w]\r\n",
    "    img2_new_face_rect_area_gray = cv2.cvtColor(img2_new_face_rect_area, cv2.COLOR_BGR2GRAY)\r\n",
    "    _, mask_triangles_designed = cv2.threshold(img2_new_face_rect_area_gray, 1, 255, cv2.THRESH_BINARY_INV)\r\n",
    "    warped_triangle = cv2.bitwise_and(warped_triangle, warped_triangle, mask=mask_triangles_designed)\r\n",
    "\r\n",
    "    img2_new_face_rect_area = cv2.add(img2_new_face_rect_area, warped_triangle)\r\n",
    "    img2_new_face[y: y + h, x: x + w] = img2_new_face_rect_area\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-RSJkvMaoOk"
   },
   "source": [
    "Here we are performing same operations as above for the destination image.\r\n",
    "After we have the triangulation of both faces we take the triangles of the source face and we extract them.\r\n",
    "We also need to take the coordinates of the triangles of the destination face, so that we can warp the triangles of the source face to have same size and perspective of the matchin triangle on the destination face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EPHGGaFkKYAE"
   },
   "outputs": [],
   "source": [
    "# Face swapped (putting 1st face into 2nd face)\r\n",
    "img2_face_mask = np.zeros_like(img2_gray)\r\n",
    "img2_head_mask = cv2.fillConvexPoly(img2_face_mask, convexhull2, 255)\r\n",
    "img2_face_mask = cv2.bitwise_not(img2_head_mask)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljukd3ECazF5"
   },
   "source": [
    "Once we have cutted and wrapped all the triangles we need to link them together.\r\n",
    "We simply rebuild the face using the triangulation pattern, with the only difference that this time we put the wrapped triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUJaa9ocKfos"
   },
   "outputs": [],
   "source": [
    "img2_head_noface = cv2.bitwise_and(img2, img2, mask=img2_face_mask)\r\n",
    "result = cv2.add(img2_head_noface, img2_new_face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P15saueta6K7"
   },
   "source": [
    "The face is now ready to be replaced. We cut out the face of the destination image to make space for the new face.\r\n",
    "\r\n",
    "So we take the new face, and the destination image without face and we link them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uyEucluKiRP"
   },
   "outputs": [],
   "source": [
    "# Creating seamless clone of two faces\r\n",
    "(x, y, w, h) = cv2.boundingRect(convexhull2)\r\n",
    "center_face2 = (int((x + x + w) / 2), int((y + y + h) / 2))\r\n",
    "seamlessclone = cv2.seamlessClone(result, img2, img2_head_mask, center_face2, cv2.NORMAL_CLONE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4DjVUa18bB4K"
   },
   "source": [
    "Finally the faces are correctly swapped and it’s time to adjust the colors so that the source image fits the destination image.\r\n",
    "\r\n",
    "On Opencv we have a built in function called “seamlessClone” that does this operation automatically.\r\n",
    "We need to take the new face (created on the 6th step), take the original destination image and it’s mask to cut out the face, we need to get the center of the face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFPEQL8PKkaP"
   },
   "outputs": [],
   "source": [
    "# Converting array to image\r\n",
    "Image.fromarray(seamlessclone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCYOaCEab6BQ"
   },
   "source": [
    "Finally we will visualize output numpy image by convert it into Image object of Pillow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQMB94U7eobp"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pFnxd8wetq_"
   },
   "source": [
    "We started with downloading the pretrained model for face landmark and download the images from the internet on which we will work. Next we used CV2 and Dlib for preprocessing the images and used different functionalities to make reach to the end which is swapping the face of destination image with source image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjNtIH_rfNiD"
   },
   "source": [
    "# Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwjh5S6LfYGD"
   },
   "source": [
    "This project can be used for learning and understanding different concepts of computer vision. This project can be use to build Augmented Reality applications like Snapchat, etc."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Face Swap.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
